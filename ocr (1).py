# -*- coding: utf-8 -*-
"""ocr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vh9ZVLAt--EHZTBnpM_Gdowjx4SQusdf
"""

import zipfile
import os

# Define the path to the uploaded zip file and the extraction directory
zip_path = '/content/OCR.zip'
extraction_dir = '/content/OCR_dataset'

# Unzip the dataset
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extraction_dir)

# List the extracted files and directories
extracted_files = os.listdir(extraction_dir)
extracted_files

# List the contents of both directories
data_contents = os.listdir(os.path.join(extraction_dir, 'data'))
data2_contents = os.listdir(os.path.join(extraction_dir, 'data2'))

data_contents, data2_contents

# List a few files from the training and testing directories in 'data'
training_data_path = os.path.join(extraction_dir, 'data', 'training_data')
testing_data_path = os.path.join(extraction_dir, 'data', 'testing_data')

training_files = os.listdir(training_data_path)[:5]
testing_files = os.listdir(testing_data_path)[:5]

training_files, testing_files

# Inspect the contents of one class folder (e.g., '0') in training data
class_0_path = os.path.join(training_data_path, '0')
class_0_files = os.listdir(class_0_path)[:5]  # List a few files from class '0'

class_0_files

import os
import cv2
import numpy as np

# Step 2: Preprocess an image
def preprocess_image(img):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img  # Convert to grayscale
    resized = cv2.resize(gray, (28, 28))  # Resize to 28x28 pixels
    blurred = cv2.GaussianBlur(resized, (3, 3), 0)  # Remove noise
    _, thresholded = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)  # Binary threshold
    normalized = thresholded / 255.0  # Normalize pixel values to [0, 1]
    return normalized

# Step 3: Preprocess all images in the dataset
def preprocess_dataset(folder_path, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            img_path = os.path.join(root, filename)
            img = cv2.imread(img_path)
            if img is not None:
                preprocessed_img = preprocess_image(img)
                # Save the preprocessed image
                label_folder = os.path.basename(root)
                save_folder = os.path.join(output_folder, label_folder)
                os.makedirs(save_folder, exist_ok=True)
                save_path = os.path.join(save_folder, filename)
                cv2.imwrite(save_path, (preprocessed_img * 255).astype(np.uint8))
    print(f"Preprocessed images saved to {output_folder}")

# Step 4: Run the preprocessing pipeline
extracted_folder = '/content/OCR_Dataset'
preprocessed_folder = '/content/OCR_Preprocessed'


print("Dataset preprocessing complete!")

import os
import cv2
import numpy as np

# Function to preprocess a single image
def preprocess_image(img):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img  # Grayscale
    resized = cv2.resize(gray, (28, 28))  # Resize to 28x28 pixels
    blurred = cv2.GaussianBlur(resized, (3, 3), 0)  # Denoising
    _, thresholded = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)  # Binarize
    normalized = thresholded / 255.0  # Normalize to [0, 1]
    return normalized

# Function to preprocess the dataset and verify saving
def preprocess_dataset(folder_path, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    processed_count = 0  # Count processed images

    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            img_path = os.path.join(root, filename)
            img = cv2.imread(img_path)
            if img is not None:
                preprocessed_img = preprocess_image(img)
                # Save preprocessed image
                label_folder = os.path.relpath(root, folder_path)
                save_folder = os.path.join(output_folder, label_folder)
                os.makedirs(save_folder, exist_ok=True)
                save_path = os.path.join(save_folder, filename)
                cv2.imwrite(save_path, (preprocessed_img * 255).astype(np.uint8))
                processed_count += 1
                print(f"Processed and saved: {save_path}")
            else:
                print(f"Failed to load image: {img_path}")

    if processed_count > 0:
        print(f"‚úÖ {processed_count} images successfully preprocessed and saved.")
    else:
        print("‚ö†Ô∏è No images were processed. Please check the dataset path.")

# Paths
dataset_folder = '/content/OCR_dataset'
preprocessed_folder = '/content/OCR_Preprocessed'

# Run preprocessing
preprocess_dataset(dataset_folder, preprocessed_folder)

import matplotlib.pyplot as plt
import os
import cv2

# Function to display sample preprocessed images
def display_sample_images(folder_path, num_samples=5):
    sample_images = []
    labels = []

    for root, dirs, files in os.walk(folder_path):
        for filename in files[:num_samples]:
            img_path = os.path.join(root, filename)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale
            if img is not None:
                sample_images.append(img)
                labels.append(os.path.basename(root))  # Use the folder name as a label

            if len(sample_images) == num_samples:
                break
        if len(sample_images) == num_samples:
            break

    # Plot the images
    if sample_images:
        plt.figure(figsize=(12, 6))
        for i, (img, label) in enumerate(zip(sample_images, labels)):
            plt.subplot(1, num_samples, i + 1)
            plt.imshow(img, cmap='gray')
            plt.title(f'Label: {label}')
            plt.axis('off')
        plt.suptitle('Sample Preprocessed Images')
        plt.show()
    else:
        print("No preprocessed images found to display.")

# Path to preprocessed dataset
preprocessed_folder = '/content/OCR_Preprocessed'

# Display sample images
display_sample_images(preprocessed_folder)

import os

# Verify that images exist in the preprocessed folder
def check_preprocessed_images(folder_path):
    for root, dirs, files in os.walk(folder_path):
        if files:
            print(f"Found {len(files)} files in {root}")
            return True
    print("No preprocessed images found in the folder.")
    return False

# Path to preprocessed folder
preprocessed_folder = '/content/OCR_Preprocessed'

# Check if images exist
check_preprocessed_images(preprocessed_folder)

import os
import cv2
import numpy as np
import pandas as pd
from skimage.feature import hog
from sklearn.preprocessing import LabelEncoder

# Function to extract HOG features from an image
def extract_hog_features(image):
    features, _ = hog(
        image,
        orientations=9,
        pixels_per_cell=(8, 8),
        cells_per_block=(2, 2),
        block_norm='L2-Hys',
        visualize=True
    )
    return features

# Function to extract features from the dataset and save as CSV
def extract_features_to_csv(folder_path, output_csv):
    features = []
    labels = []
    le = LabelEncoder()  # Convert string labels to integers

    for root, dirs, files in os.walk(folder_path):
        label = os.path.basename(root)
        for filename in files:
            img_path = os.path.join(root, filename)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale
            if img is not None:
                hog_features = extract_hog_features(img)
                features.append(hog_features)
                labels.append(label)

    # Convert labels to integers
    encoded_labels = le.fit_transform(labels)

    # Convert to DataFrame and save as CSV
    df = pd.DataFrame(features)
    df["Label"] = encoded_labels  # Add labels to the last column
    df.to_csv(output_csv, index=False)

    print(f"‚úÖ Features saved to {output_csv}")

# Path to preprocessed dataset and CSV output file
preprocessed_folder = "/content/OCR_Preprocessed"
output_csv_file = "/content/OCR_Features.csv"

# Extract features and save to CSV
extract_features_to_csv(preprocessed_folder, output_csv_file)

import matplotlib.pyplot as plt
import random
from skimage.feature import hog

# Function to visualize original image and its HOG feature visualization
def display_hog_features(folder_path, num_samples=5):
    sample_images = []

    # Collect all image paths
    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            img_path = os.path.join(root, filename)
            sample_images.append(img_path)

    # Randomly select 5 images
    random_samples = random.sample(sample_images, num_samples)

    # Plot original images and their HOG visualizations
    plt.figure(figsize=(15, 8))
    for i, img_path in enumerate(random_samples):
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is not None:
            # Extract HOG features and visualization
            _, hog_image = hog(
                img,
                orientations=9,
                pixels_per_cell=(8, 8),
                cells_per_block=(2, 2),
                block_norm='L2-Hys',
                visualize=True
            )

            # Display original image
            plt.subplot(num_samples, 2, 2 * i + 1)
            plt.imshow(img, cmap='gray')
            plt.title(f'Original Image {i+1}')
            plt.axis('off')

            # Display HOG feature image
            plt.subplot(num_samples, 2, 2 * i + 2)
            plt.imshow(hog_image, cmap='gray')
            plt.title(f'HOG Features {i+1}')
            plt.axis('off')

    plt.tight_layout()
    plt.show()

# Path to preprocessed folder
display_hog_features(preprocessed_folder)

import pandas as pd
from sklearn.decomposition import PCA

# Function to apply PCA on extracted features
def apply_pca(features, n_components=100):
    pca = PCA(n_components=n_components)
    reduced_features = pca.fit_transform(features)
    print(f"‚úÖ PCA applied: Reduced from {features.shape[1]} to {n_components} dimensions.")
    return reduced_features, pca

# Load extracted features from CSV
features_df = pd.read_csv("/content/OCR_Features.csv")

# Extract labels separately
labels = features_df["Label"].values  # Assuming the last column is the label
features = features_df.drop(columns=["Label"]).values  # Drop label column

# Apply PCA to reduce dimensions
reduced_features, pca_model = apply_pca(features, n_components=30)

# Convert reduced features into a DataFrame
pca_df = pd.DataFrame(reduced_features)
pca_df["Label"] = labels  # Add labels back to the dataset

# Save PCA features to CSV
pca_df.to_csv("/content/OCR_PCA_Features.csv", index=False)
print("‚úÖ PCA-reduced features saved to CSV.")

# Plot explained variance to determine the number of components needed
import matplotlib.pyplot as plt
import numpy as np

def plot_explained_variance(pca):
    plt.figure(figsize=(10, 5))
    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('Explained Variance by PCA Components')
    plt.grid(True)
    plt.show()

# Plot variance explained by PCA
plot_explained_variance(pca_model)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load PCA-reduced features from CSV
pca_df = pd.read_csv("/content/OCR_PCA_Features.csv")

# Separate features and labels
labels = pca_df["Label"].values  # Extract labels
features = pca_df.drop(columns=["Label"]).values  # Drop label column

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42, stratify=labels)

# 1Ô∏è‚É£ **Support Vector Machine (SVM)**
svm_model = SVC(kernel="linear", C=1, random_state=42)
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

# 2Ô∏è‚É£ **K-Nearest Neighbors (KNN)**
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_predictions = knn_model.predict(X_test)

# 3Ô∏è‚É£ **Random Forest (RF)**
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# 4Ô∏è‚É£ **Na√Øve Bayes**
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
nb_predictions = nb_model.predict(X_test)

# ‚úÖ **Evaluation**
models = {
    "SVM": svm_predictions,
    "KNN": knn_predictions,
    "Random Forest": rf_predictions,
    "Na√Øve Bayes": nb_predictions
}

for model_name, predictions in models.items():
    print(f"\nüîç {model_name} Model Evaluation:")
    print("Accuracy:", accuracy_score(y_test, predictions))
    print("Classification Report:\n", classification_report(y_test, predictions))

print("\n‚úÖ Model training and evaluation completed.")

# ================================
# DEEP LEARNING MODEL (ANN)
# ================================

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam

# -------------------------------
# Normalize features
# -------------------------------
X_train_dl = X_train / np.max(X_train)
X_test_dl = X_test / np.max(X_test)

# -------------------------------
# Convert labels to categorical
# -------------------------------
num_classes = len(np.unique(y_train))

y_train_cat = to_categorical(y_train, num_classes)
y_test_cat = to_categorical(y_test, num_classes)

# -------------------------------
# ANN Model
# -------------------------------
ann_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_dl.shape[1],)),
    Dropout(0.3),

    Dense(64, activation='relu'),
    Dropout(0.2),

    Dense(num_classes, activation='softmax')
])

# -------------------------------
# Compile model
# -------------------------------
ann_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# -------------------------------
# Train model
# -------------------------------
history = ann_model.fit(
    X_train_dl,
    y_train_cat,
    epochs=30,
    batch_size=32,
    validation_data=(X_test_dl, y_test_cat),
    verbose=1
)

# -------------------------------
# Evaluate model
# -------------------------------
loss, acc = ann_model.evaluate(X_test_dl, y_test_cat, verbose=0)

print("\n Deep Learning ANN Accuracy:", acc)